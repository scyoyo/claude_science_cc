"""
Exporter: Export meeting artifacts in various formats.

Supports:
- ZIP download: Bundle all code artifacts into a zip file (with subdirectories)
- GitHub push preparation: Generate file structure for GitHub
- Google Colab notebook: Convert code artifacts to .ipynb format
"""

import io
import json
import zipfile
from typing import List

from app.core.code_extractor import generate_requirements
from app.schemas.artifact import CodeArtifactResponse


def _build_directory_tree(filenames: List[str]) -> str:
    """Build a text-based directory tree from a list of file paths.

    Example output:
        src/
          models/
            pipeline.py
            utils.py
          main.py
        tests/
          test_pipeline.py
        requirements.txt
    """
    tree: dict = {}
    for name in sorted(filenames):
        parts = name.split("/")
        node = tree
        for part in parts:
            node = node.setdefault(part, {})

    lines: List[str] = []

    def _render(node: dict, indent: int = 0) -> None:
        for key in sorted(node.keys()):
            children = node[key]
            prefix = "  " * indent
            if children:
                lines.append(f"{prefix}{key}/")
                _render(children, indent + 1)
            else:
                lines.append(f"{prefix}{key}")

    _render(tree)
    return "\n".join(lines)


def export_as_zip(
    artifacts: List[dict],
    project_name: str = "virtual_lab_export",
) -> bytes:
    """Export artifacts as a ZIP file.

    Supports subdirectory structure via '/' in filenames.
    Automatically generates requirements.txt from Python imports.

    Args:
        artifacts: List of artifact dicts with 'filename', 'content', 'language' keys.
        project_name: Name of the root directory in the zip.

    Returns:
        Bytes of the ZIP file.
    """
    buffer = io.BytesIO()
    with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED) as zf:
        # Generate requirements.txt
        requirements = generate_requirements(artifacts)

        # Build file list for README
        filenames = [a["filename"] for a in artifacts]
        if requirements:
            filenames.append("requirements.txt")

        # Build README with directory tree
        readme = f"# {project_name}\n\nGenerated by Virtual Lab\n\n## Project Structure\n\n```\n"
        readme += _build_directory_tree(filenames)
        readme += "\n```\n"
        zf.writestr(f"{project_name}/README.md", readme)

        # Add each artifact (subdirectories are created automatically by zipfile)
        for a in artifacts:
            zf.writestr(f"{project_name}/{a['filename']}", a["content"])

        # Add requirements.txt if any packages detected
        if requirements:
            zf.writestr(f"{project_name}/requirements.txt", requirements)

    return buffer.getvalue()


def export_as_colab_notebook(
    artifacts: List[dict],
    project_name: str = "virtual_lab_export",
) -> dict:
    """Export artifacts as a Google Colab-compatible Jupyter notebook.

    Since notebooks don't support directories, file paths are annotated
    in markdown cells above each code cell.

    Args:
        artifacts: List of artifact dicts.
        project_name: Name for the notebook title.

    Returns:
        Dict representing the .ipynb JSON structure.
    """
    cells = []

    # Title cell
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            f"# {project_name}\n",
            "\n",
            "Generated by Virtual Lab\n",
        ],
    })

    # One cell per artifact
    for a in artifacts:
        # Description markdown cell with filepath annotation
        desc = a.get("description", "")
        source_lines = [f"## {a['filename']}\n"]
        if "/" in a["filename"]:
            source_lines.append(f"\n> Target path: `{a['filename']}`\n")
        if desc:
            source_lines.append(f"\n{desc}\n")

        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": source_lines,
        })

        # Code cell
        cell_type = "code" if a["language"] in ("python", "py") else "code"
        code_source_lines = a["content"].split("\n")
        source_with_newlines = [line + "\n" for line in code_source_lines[:-1]]
        if code_source_lines:
            source_with_newlines.append(code_source_lines[-1])

        cells.append({
            "cell_type": cell_type,
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": source_with_newlines,
        })

    # Add requirements install cell if needed
    requirements = generate_requirements(artifacts)
    if requirements:
        pip_packages = " ".join(requirements.split("\n"))
        cells.insert(1, {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [f"!pip install {pip_packages}\n"],
        })

    notebook = {
        "nbformat": 4,
        "nbformat_minor": 0,
        "metadata": {
            "colab": {"name": f"{project_name}.ipynb"},
            "kernelspec": {
                "name": "python3",
                "display_name": "Python 3",
            },
            "language_info": {"name": "python"},
        },
        "cells": cells,
    }

    return notebook


def export_as_github_files(
    artifacts: List[dict],
    project_name: str = "virtual_lab_export",
) -> List[dict]:
    """Prepare artifacts as a list of files for GitHub push.

    Supports subdirectory paths in filenames.

    Args:
        artifacts: List of artifact dicts.
        project_name: Repository/project name.

    Returns:
        List of dicts with 'path' and 'content' keys ready for GitHub API.
    """
    files = []

    # Generate requirements.txt
    requirements = generate_requirements(artifacts)

    # Build file list for README
    filenames = [a["filename"] for a in artifacts]
    if requirements:
        filenames.append("requirements.txt")

    # README with directory tree
    readme = f"# {project_name}\n\nGenerated by Virtual Lab\n\n## Project Structure\n\n```\n"
    readme += _build_directory_tree(filenames)
    readme += "\n```\n"
    files.append({"path": "README.md", "content": readme})

    # Artifacts (path supports subdirectories)
    for a in artifacts:
        files.append({"path": a["filename"], "content": a["content"]})

    # Requirements
    if requirements:
        files.append({"path": "requirements.txt", "content": requirements})

    return files


def export_as_paper(
    meeting_title: str,
    summary_text: str | None,
    key_points: list | None,
    transcript_lines: list[str],
    artifact_dicts: List[dict],
) -> str:
    """Export meeting as an academic-style paper (markdown)."""
    lines = [
        f"# {meeting_title}",
        "",
        "## Abstract",
        "",
        (summary_text or "No summary available.").strip(),
        "",
    ]
    if key_points:
        lines.append("## Key Points")
        lines.append("")
        for p in key_points:
            lines.append(f"- {p}")
        lines.append("")
    if transcript_lines:
        lines.append("## Discussion")
        lines.append("")
        lines.extend(transcript_lines)
        lines.append("")
    if artifact_dicts:
        lines.append("## Appendix: Code and Artifacts")
        lines.append("")
        for a in artifact_dicts:
            lines.append(f"### {a['filename']}")
            lines.append("")
            lines.append("```" + (a.get("language") or "text"))
            lines.append(a.get("content", ""))
            lines.append("```")
            lines.append("")
    return "\n".join(lines)


def export_as_blog(
    meeting_title: str,
    summary_text: str | None,
    transcript_lines: list[str],
    artifact_dicts: List[dict],
) -> str:
    """Export meeting as a tech blog post (markdown)."""
    lines = [
        f"# {meeting_title}",
        "",
        (summary_text or "").strip() or "This post summarizes the discussion and outputs from the meeting.",
        "",
        "---",
        "",
    ]
    if transcript_lines:
        lines.append("## Discussion")
        lines.append("")
        lines.extend(transcript_lines)
        lines.append("")
    if artifact_dicts:
        lines.append("## Code")
        lines.append("")
        for a in artifact_dicts:
            lines.append(f"**{a['filename']}**")
            lines.append("")
            lines.append("```" + (a.get("language") or "text"))
            lines.append(a.get("content", ""))
            lines.append("```")
            lines.append("")
    return "\n".join(lines)
